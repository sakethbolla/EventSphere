name: Deploy to Staging (kind)

on:
  workflow_run:
    workflows: ["Build and Push Docker Images"]
    types:
      - completed
    branches:
      - main
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Image tag to deploy (default: latest)'
        required: false
        default: 'latest'

env:
  REGISTRY: ghcr.io
  IMAGE_PREFIX: ${{ github.repository }}
  CLUSTER_NAME: eventsphere-test

jobs:
  deploy-test:
    name: Deploy to Staging (kind)
    runs-on: ubuntu-latest
    # Only run after successful builds or manual dispatch
    if: |
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success') ||
      github.event_name == 'workflow_dispatch'
    permissions:
      contents: read
      packages: read
      id-token: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_sha || github.ref }}

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Set up kind
        uses: helm/kind-action@v1.10.0
        with:
          version: 'v0.20.0'

      - name: Create kind cluster
        run: |
          kind create cluster --name ${{ env.CLUSTER_NAME }} --wait 5m
          kubectl cluster-info --context kind-${{ env.CLUSTER_NAME }}

      - name: Set image tag
        id: set-tag
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "tag=${{ github.event.inputs.image_tag }}" >> $GITHUB_OUTPUT
          else
            # Use commit SHA for all builds (matches build workflow)
            echo "tag=${{ github.event.workflow_run.head_sha }}" >> $GITHUB_OUTPUT
          fi

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Install Cosign
        uses: sigstore/cosign-installer@v3
        with:
          cosign-release: 'v2.2.1'

      - name: Verify and pull images from GHCR
        run: |
          IMAGE_TAG="${{ steps.set-tag.outputs.tag }}"
          IMAGE_PREFIX=$(echo '${{ env.IMAGE_PREFIX }}' | tr '[:upper:]' '[:lower:]')
          echo "Verifying and pulling images with tag: ${IMAGE_TAG}"
          
          # Verify and pull each image
          for service in auth-service event-service booking-service frontend; do
            IMAGE_REF="${{ env.REGISTRY }}/${IMAGE_PREFIX}/${service}:${IMAGE_TAG}"
            echo "Verifying signature for ${IMAGE_REF}..."
            if cosign verify --certificate-identity-regexp=".*" --certificate-oidc-issuer="https://token.actions.githubusercontent.com" ${IMAGE_REF}; then
              echo "✓ Signature verified for ${service}"
              docker pull ${IMAGE_REF}
            else
              echo "✗ Signature verification failed for ${service}"
              exit 1
            fi
          done

      - name: Load images into kind cluster
        run: |
          IMAGE_TAG="${{ steps.set-tag.outputs.tag }}"
          IMAGE_PREFIX=$(echo '${{ env.IMAGE_PREFIX }}' | tr '[:upper:]' '[:lower:]')
          kind load docker-image --name ${{ env.CLUSTER_NAME }} \
            ${{ env.REGISTRY }}/${IMAGE_PREFIX}/auth-service:${IMAGE_TAG}
          kind load docker-image --name ${{ env.CLUSTER_NAME }} \
            ${{ env.REGISTRY }}/${IMAGE_PREFIX}/event-service:${IMAGE_TAG}
          kind load docker-image --name ${{ env.CLUSTER_NAME }} \
            ${{ env.REGISTRY }}/${IMAGE_PREFIX}/booking-service:${IMAGE_TAG}
          kind load docker-image --name ${{ env.CLUSTER_NAME }} \
            ${{ env.REGISTRY }}/${IMAGE_PREFIX}/frontend:${IMAGE_TAG}

      - name: Verify NET_BIND_SERVICE capability in frontend image before deployment
        run: |
          IMAGE_TAG="${{ steps.set-tag.outputs.tag }}"
          IMAGE_PREFIX=$(echo '${{ env.IMAGE_PREFIX }}' | tr '[:upper:]' '[:lower:]')
          FRONTEND_IMAGE="${{ env.REGISTRY }}/${IMAGE_PREFIX}/frontend:${IMAGE_TAG}"
          
          echo "Verifying NET_BIND_SERVICE capability in frontend image: ${FRONTEND_IMAGE}"
          echo "This ensures the image has the capability set before deployment."
          
          # Check capability in the image that was just loaded
          CAP_OUTPUT=$(docker run --rm --platform linux/amd64 --entrypoint sh "${FRONTEND_IMAGE}" -c "getcap /usr/sbin/nginx" 2>&1)
          
          echo "Capability check output:"
          echo "${CAP_OUTPUT}"
          
          if echo "${CAP_OUTPUT}" | grep -q "cap_net_bind_service"; then
            echo "✅ SUCCESS: NET_BIND_SERVICE capability is set on nginx binary in the image"
            echo "Expected output: /usr/sbin/nginx = cap_net_bind_service+ep"
          else
            echo "❌ ERROR: NET_BIND_SERVICE capability NOT found on nginx binary in the image!"
            echo "This image was likely built before the Dockerfile was updated with setcap."
            echo "The image must be rebuilt with the updated Dockerfile that includes:"
            echo "  RUN apk add --no-cache libcap && setcap 'cap_net_bind_service=+ep' /usr/sbin/nginx"
            echo ""
            echo "Current image tag: ${IMAGE_TAG}"
            echo "Please ensure the build workflow ran successfully for this commit SHA."
            exit 1
          fi

      - name: Tag images for Kubernetes
        run: |
          IMAGE_TAG="${{ steps.set-tag.outputs.tag }}"
          IMAGE_PREFIX=$(echo '${{ env.IMAGE_PREFIX }}' | tr '[:upper:]' '[:lower:]')
          
          # Ensure images are properly tagged in containerd for Kubernetes
          # This step ensures images loaded by kind are accessible with their full registry path
          # matching what's specified in the deployment manifests (matches local test script approach)
          for service in auth-service event-service booking-service frontend; do
            FULL_IMAGE="${{ env.REGISTRY }}/${IMAGE_PREFIX}/${service}:${IMAGE_TAG}"
            
            # Verify image is accessible (kind should have loaded it with full path)
            # If needed, create additional tag to ensure Kubernetes can find it
            # This is a safety measure to match the local testing approach
            if ! docker exec ${{ env.CLUSTER_NAME }}-control-plane ctr -n=k8s.io images ls 2>/dev/null | grep -q "${FULL_IMAGE}"; then
              echo "⚠️ Image ${FULL_IMAGE} not found in containerd, attempting to tag..."
              # Try to tag from docker.io/library if kind loaded it that way
              docker exec ${{ env.CLUSTER_NAME }}-control-plane ctr -n=k8s.io images tag "docker.io/library/${service}:${IMAGE_TAG}" "${FULL_IMAGE}" 2>&1 | grep -v "^$" || true
            fi
          done
          
          echo "✅ Images verified for Kubernetes access"

      - name: Process Kubernetes templates
        run: |
          # Install gettext for envsubst
          sudo apt-get update && sudo apt-get install -y gettext-base
          
          # Set environment variables for template processing
          # Use GHCR registry for kind deployment
          export ECR_REGISTRY="${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}"
          export AWS_REGION="us-east-1"
          export CLUSTER_NAME="${{ env.CLUSTER_NAME }}"
          export ACM_CERTIFICATE_ARN=""
          export FLUENT_BIT_ROLE_ARN=""
          export EXTERNAL_SECRETS_ROLE_ARN=""
          
          # Create output directory
          mkdir -p k8s/generated/base
          mkdir -p k8s/generated/ingress
          mkdir -p k8s/generated/overlays/dev
          
          # Process template files
          find k8s -name "*.template" -type f | while read template; do
            output_file=$(echo "$template" | sed 's/\.template$//' | sed 's|k8s/|k8s/generated/|')
            output_dir=$(dirname "$output_file")
            mkdir -p "$output_dir"
            envsubst < "$template" > "$output_file"
            echo "Processed: $template -> $output_file"
          done
          
          # Copy non-template files
          cp -r k8s/base/*.yaml k8s/generated/base/ 2>/dev/null || true
          cp -r k8s/ingress/*.yaml k8s/generated/ingress/ 2>/dev/null || true
          cp -r k8s/overlays/dev/*.yaml k8s/generated/overlays/dev/ 2>/dev/null || true

      - name: Update image tags in deployments
        run: |
          IMAGE_PREFIX=$(echo '${{ env.IMAGE_PREFIX }}' | tr '[:upper:]' '[:lower:]')
          IMAGE_REGISTRY="${{ env.REGISTRY }}/${IMAGE_PREFIX}"
          IMAGE_TAG="${{ steps.set-tag.outputs.tag }}"
          
          # Update image tags in deployment files
          # Also set imagePullPolicy to IfNotPresent since images are loaded into kind
          find k8s/generated -name "*deployment.yaml" -type f | while read file; do
            sed -i "s|image:.*auth-service.*|image: ${IMAGE_REGISTRY}/auth-service:${IMAGE_TAG}|g" "$file"
            sed -i "s|image:.*event-service.*|image: ${IMAGE_REGISTRY}/event-service:${IMAGE_TAG}|g" "$file"
            sed -i "s|image:.*booking-service.*|image: ${IMAGE_REGISTRY}/booking-service:${IMAGE_TAG}|g" "$file"
            sed -i "s|image:.*frontend.*|image: ${IMAGE_REGISTRY}/frontend:${IMAGE_TAG}|g" "$file"
            # Set imagePullPolicy to IfNotPresent to use loaded images in kind
            sed -i "s|imagePullPolicy: Always|imagePullPolicy: IfNotPresent|g" "$file"
          done
          
          echo "Updated image tags to: ${IMAGE_REGISTRY}/*:${IMAGE_TAG}"
          echo "Set imagePullPolicy to IfNotPresent for kind deployment"

      - name: Create test namespace
        run: |
          kubectl create namespace dev --dry-run=client -o yaml | kubectl apply -f -

      - name: Create MongoDB secrets
        run: |
          # Create MongoDB secret with proper connection string
          MONGODB_PASSWORD="test-mongodb-password"
          MONGO_CONNECTION_STRING="mongodb://admin:${MONGODB_PASSWORD}@mongodb.dev.svc.cluster.local:27017/eventsphere?authSource=admin"
          
          kubectl create secret generic mongodb-secret \
            --from-literal=username=admin \
            --from-literal=password="${MONGODB_PASSWORD}" \
            --from-literal=connection-string="${MONGO_CONNECTION_STRING}" \
            -n dev --dry-run=client -o yaml | kubectl apply -f -
          
          # Create auth service secret
          kubectl create secret generic auth-service-secret \
            --from-literal=jwt-secret="test-jwt-secret-key-for-kind-deployment" \
            -n dev --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy MongoDB for testing
        run: |
          # Create MongoDB service
          kubectl apply -f - <<EOF
          apiVersion: v1
          kind: Service
          metadata:
            name: mongodb
            namespace: dev
          spec:
            type: ClusterIP
            ports:
            - port: 27017
              targetPort: 27017
            selector:
              app: mongodb
          EOF
          
          # Create MongoDB deployment (simplified for kind - no persistent storage)
          kubectl apply -f - <<EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: mongodb
            namespace: dev
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: mongodb
            template:
              metadata:
                labels:
                  app: mongodb
              spec:
                containers:
                - name: mongodb
                  image: mongo:7.0
                  ports:
                  - containerPort: 27017
                  env:
                  - name: MONGO_INITDB_ROOT_USERNAME
                    valueFrom:
                      secretKeyRef:
                        name: mongodb-secret
                        key: username
                  - name: MONGO_INITDB_ROOT_PASSWORD
                    valueFrom:
                      secretKeyRef:
                        name: mongodb-secret
                        key: password
                  - name: MONGO_INITDB_DATABASE
                    value: "eventsphere"
                  resources:
                    requests:
                      memory: "256Mi"
                      cpu: "250m"
                    limits:
                      memory: "512Mi"
                      cpu: "500m"
          EOF
          
          # Wait for MongoDB to be ready
          echo "Waiting for MongoDB to be ready..."
          kubectl wait --for=condition=available --timeout=120s \
            deployment/mongodb -n dev || {
            echo "MongoDB deployment not ready"
            kubectl describe deployment mongodb -n dev
            kubectl logs -n dev -l app=mongodb --tail=50
            exit 1
          }
          
          echo "MongoDB is ready"

      - name: Update namespace in generated manifests
        run: |
          # Update namespace from prod to dev in all generated files
          find k8s/generated -name "*.yaml" -type f -exec sed -i 's/namespace: prod/namespace: dev/g' {} \;
          find k8s/generated -name "*.yaml" -type f -exec sed -i 's/namespace: staging/namespace: dev/g' {} \;

      - name: Install Kyverno CLI
        uses: kyverno/action-install-cli@v0.2.0

      - name: Validate Kubernetes manifests with Kyverno
        run: |
          echo "Validating Kubernetes manifests with Kyverno policies..."
          
          # Validate all generated manifests against policies
          # Create a temporary directory for validation
          mkdir -p /tmp/kyverno-validation
          
          # Copy all manifests to validate
          for manifest_dir in k8s/generated/base k8s/generated/ingress; do
            if [ -d "$manifest_dir" ]; then
              echo "Validating manifests in $manifest_dir..."
              find "$manifest_dir" -name "*.yaml" -type f -exec cp {} /tmp/kyverno-validation/ \;
            fi
          done
          
          # Run Kyverno validation on all manifests
          if [ "$(ls -A /tmp/kyverno-validation)" ]; then
            echo "Running Kyverno policy validation..."
            if ! kyverno apply k8s/security/kyverno-policies.yaml --resource /tmp/kyverno-validation/; then
              echo "❌ Kyverno validation failed"
              echo "Review the output above for policy violations"
              exit 1
            fi
            echo "✅ All manifests passed Kyverno validation"
          else
            echo "⚠️ No manifests found to validate"
          fi

      - name: Verify frontend deployment has NET_BIND_SERVICE capability
        run: |
          echo "Checking frontend deployment for NET_BIND_SERVICE capability..."
          if grep -q "NET_BIND_SERVICE" k8s/generated/base/frontend-deployment.yaml; then
            echo "✅ NET_BIND_SERVICE capability found in frontend deployment"
            echo "Frontend deployment securityContext:"
            grep -A 10 "securityContext:" k8s/generated/base/frontend-deployment.yaml | head -15
          else
            echo "❌ NET_BIND_SERVICE capability NOT found in frontend deployment!"
            echo "Full securityContext section:"
            grep -A 10 "securityContext:" k8s/generated/base/frontend-deployment.yaml || echo "No securityContext found"
            exit 1
          fi

      - name: Apply ConfigMaps
        run: |
          kubectl apply -f k8s/generated/base/configmaps.yaml -n dev || true

      - name: Apply RBAC
        run: |
          kubectl apply -f k8s/generated/base/rbac.yaml || true

      - name: Apply Deployments and Services
        run: |
          kubectl apply -f k8s/generated/base/ -n dev

      - name: Wait for deployments to be ready
        run: |
          echo "Waiting for deployments to be ready..."
          for deployment in auth-service event-service booking-service frontend; do
            if kubectl get deployment "$deployment" -n dev &> /dev/null; then
              echo "Waiting for $deployment..."
              kubectl wait --for=condition=available --timeout=300s \
                deployment/"$deployment" -n dev || {
                echo "Deployment $deployment not ready within timeout"
                kubectl describe deployment "$deployment" -n dev
                kubectl logs -n dev -l app="$deployment" --tail=50
                
                # For frontend, also check pod security context and verify capability in running pod
                if [ "$deployment" == "frontend" ]; then
                  echo ""
                  echo "=== Frontend Pod Security Context ==="
                  kubectl get pods -n dev -l app=frontend -o yaml | grep -A 20 "securityContext:" || true
                  echo ""
                  echo "=== Frontend Pod Capabilities ==="
                  kubectl get pods -n dev -l app=frontend -o yaml | grep -A 10 "capabilities:" || true
                  echo ""
                  echo "=== Verifying NET_BIND_SERVICE capability on nginx binary in running pod ==="
                  FRONTEND_POD=$(kubectl get pods -n dev -l app=frontend -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
                  if [ -n "$FRONTEND_POD" ]; then
                    echo "Checking capability on nginx binary in pod: $FRONTEND_POD"
                    kubectl exec -n dev "$FRONTEND_POD" -- getcap /usr/sbin/nginx 2>&1 || echo "Failed to check capability (pod may not be running)"
                    echo ""
                    echo "Checking process capabilities:"
                    kubectl exec -n dev "$FRONTEND_POD" -- sh -c "cat /proc/self/status | grep Cap" 2>&1 || echo "Failed to check process capabilities"
                    echo ""
                    echo "Checking if nginx binary exists and is executable:"
                    kubectl exec -n dev "$FRONTEND_POD" -- ls -la /usr/sbin/nginx 2>&1 || echo "Failed to check nginx binary"
                  else
                    echo "No frontend pod found to check"
                  fi
                fi
                
                exit 1
              }
              echo "$deployment is ready"
            fi
          done

      - name: Verify deployment
        run: |
          echo "=== Deployment Status ==="
          kubectl get deployments -n dev
          echo ""
          echo "=== Pod Status ==="
          kubectl get pods -n dev
          echo ""
          echo "=== Service Status ==="
          kubectl get services -n dev
          echo ""
          echo "=== Pod Logs (last 20 lines) ==="
          for pod in $(kubectl get pods -n dev -o jsonpath='{.items[*].metadata.name}'); do
            echo "--- Logs for $pod ---"
            kubectl logs -n dev "$pod" --tail=20 || true
          done

      - name: Test service endpoints
        run: |
          echo "Testing service endpoints..."
          
          # Port forward services and test
          kubectl port-forward -n dev svc/auth-service 4001:4001 &
          PORT_FWD_PID=$!
          sleep 5
          
          # Test health endpoint
          if curl -f http://localhost:4001/health > /dev/null 2>&1; then
            echo "✓ auth-service health check passed"
          else
            echo "✗ auth-service health check failed"
          fi
          
          kill $PORT_FWD_PID 2>/dev/null || true
          
          echo "Note: Full integration testing requires MongoDB. This test validates deployment structure."

      - name: Cleanup kind cluster
        if: always()
        run: |
          kind delete cluster --name ${{ env.CLUSTER_NAME }} || true

